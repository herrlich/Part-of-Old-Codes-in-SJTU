%%==========================
%% chapter01.tex for SJTU Bachelor Thesis
%% version: 0.5.2
%% Encoding: UTF-8
%%==================================================

%\bibliographystyle{sjtu2} %[此处用于每章都生产参考文献]
\pagestyle{plain}
\renewcommand{\chaptername}{Chapter \CJKnumber{\thechapter}}

\chapter{Introduction}
\label{chap:intro}

Currently, the structure and function of the Internet has become more and more complex, as a result of the rapid expansion of the Internet scale. Meanwhile, the hardwares in the network are given more and more responsiblities. For example, the router, which should have been a simple data forwarding unit, has now taken control of filtering packets, distinguishing services, and deciding the quality of service. However, the coupling of logical controlling and data forwarding on the hardware makes it very hard to manage the control plane of the Internet, and thus hindering the deployment of new flexible structures and techologies.

To solve this problem existed in the widely used Internet structure, several projects have been raised, such as Global Environment for Network Innovations(GENI)\cite{geni} of US, Future Internet Research and Experimentation Initative(FIRE)\cite{fire} of EU, AKARI \& JGN2plus\cite{akari} of Japan, and Smart Applications on Virtual Infrastructure(SAVI)\cite{savi} of Canada. They all aim at exploring new Internet structures, so that researchers are allowed to test their ideas on a virtualized network. Among all these kinds of projects, though not born for network virtualization on purpose, OpenFlow\cite{openflow} brings endless posibilities on the development of future network.

OpenFlow has its spring in Clean State Project Team of Stanford University. The final purpose of the Clean State program is to reinvent the Internet, aiming at changing the basic structure of the current network, which is out-of-date and is hard to develop. Martin Casado, a student of Stanford, lead the project Ethane\cite{ethane} which is related to network security and management in 2006. The project attempted to allow network administrator to design flow-based safety control policies conveniently, and apply these security policies to various network devices, so that researchers can realize the safety control of the whole network, by using a centralized controller. Shortly afterwards, inspired by Ethane and its predeccessor SANE\cite{sane}, Martin and his supervisor found that the design of Ethane can be generalized. By separating the control plane and data plane, and using standard application programming interface of a centralized controller to configure and manage the network devices, it is possible to make the design and management of the network more flexible, and promote the development of Internet. Thus the concept of OpenFlow was raised, and was introduced in detail in the paper -- OpenFlow: Enabling Innovation in Campus Networks\cite{openflow}, published on SIGCOMM, 2008. 

Apart from elaborating the operational principle of OpenFlow, the paper also listed several applicable situations of OpenFlow. Then later Nick and his team put forward the concept of Software Defined Network(SDN), based on the programmable character OpenFlow brings to the network. If all the devices in the network can be regarded as managable resources, then it is possible to abstract a network operating system(OS) according to the principle of OS. Then users can develop various applications on this platform, and satisfy different requirements of network resources by defining logical network topology, without caring about the physical topology.

In fact, though OpenFlow and SDN drew a great deal of attention in the industry after they are published, the idea of programmable network and separating the control plane and data plane can be traced far to the past. In 1995, a serious of activities were launched by the open Signaling Working Group(OPENSIG), trying to make the Internet more flexible and easier to program. Later, Active Network first came up with the idea of programmable network basic devices, which was intended to be used in custom services. Then the project team of forwarding and control element separation(ForCES) divides the network element to control element and forwarding element, based on programmable network structure. Though the inside structure was redefined in this way, the control element and the forwarding element were still presented as a single network unit. To solve the problem that the control plane and the devices are in a tight coupling, Greenberg and his assistants redesigned the control and management structure of the Internet, and raised a new system structure called 4D(decision, dissemination, discovery, data)\cite{4d1, 4d2} . Under the 4D structure, these four function planes are separated, and make the management of the network clean and clear. In fact, SANE, the predecessor of Ethane, also used 4D structure as its designing principle.

Briefly speaking, OpenFlow is in fact a kind of communication protocol between controllers and switches\cite{openbook}, which provides us with the right to visit the data plane. Meanwhile, OpenFlow can also be regarded as a new network communication model, which allows deciding the forwarding route of network switches by simply running an application on the server. This seperation of the control plane and the data plane makes it easier to manage the traffic in the system.

However, though we take the control plane out of physical devices, the scalability of the control plane is still constricted, mainly for the following reasons: (1) Current controllers have to deal with many flow requests in detail, and this requires the controllers responding to more requests. In an OpenFlow network, installing the flow tables in advance will also waste a lot of space, and in fact, many flows only last for little time. (2) The control plane is carrying on more and more functions, such as traffic control, load balance and switch migration. Thus the control plane has to execute all these management functions, and that also increase the processing cost. (3) With the scale of network and forwarding devices increasing dramatically, a single controller may not fulfill the performance requirements, and will easily become the bottleneck if the system requires a high performance. Among all these problems, the last one is extremely obvious in the OpenFlow system.

An OpenFlow network is mainly composed of controller, OpenFlow switch, and OpenFlow standard protocol by which the controller and the switches communicates with each other. For general DCNs that use the OpenFlow structure, a centralized controller is usually used to control the whole network. However, though this kind of method is easy to implement, it will lead to several problems if the scale of the network expands rapidly. First, the load that should be processed by this single controller will increase if the number of switches increases, but there is limited bandwidth between a switch and the controller. That is to say, if the control traffic exceeds the bandwidth, the respond time will become far longer. Second, if the scale of DCN is very large, then it will be hard for the controller to communicate with a switch that is far away from it, and will affect the forwarding speed of the data plane. The third is that, since the performance of the controller has a upper bound, it will become the bottleneck of the whole network if the amount of load exceeds its limit.

Thus comes the concept of devolved controller\cite{devolved}. In the design of devolved controller, several independent controllers are used to manipulate the network system, other than a single centralized controller that can get all information of the current status. Each independent controller thus will be only responsible for a part of the work, and by these controllers working together, the whole network can realize its normal function. In this way, we can avoid the problem that, when real time traffic are to be processed by the controllers, a quite large amount of data would cause the data processing slow. This is quite similar to the 'divide and conquer' concept in programming. In the OpenFlow system, the switches can be divided into several parts. Then we can use multiple controllers as in \cite{devolved,devolvedglobecom,multictr,elasticsdn}, and make each of them cover a portion of the traffic or computations, thus avoid the congestion. Meanwhile, by linking multiple controllers to a switch, the system becomes more stable in condition that the master controller is down because of some physical reasons.

In this condition, another problem is to be solved -- load balance\cite{lwc}. In a DCN that deploys the OpenFlow structure, if multiple controllers are used, there is a high possiblity that the load on each controller is very different, thus causing unbalancing problem in the whole system. In other words, though the global bottleneck disappears as a result of deploying devolved controllers, local bottleneck may still exist. Thus the necessity of paying attention to load balance is quite obvious. Load balance is a load sharing strategy, and is also a very important aspect in the research of distributed systems. Its main purpose is to uniformly distribute the received requests to several operating units, such as Web servers, so that the current resource ultilization can be optimized. Meanwhile, by applying adequate load balancing schemes to the system, the response time required for processing data can be reduced, and the throughput can approach its maximum. 

Since there are multiple controllers in this load balance model, synchronization problem should also be considered very carefully. Levin and his partners studied the problem of synchronization load balancer and presented two implements\cite{localcen}. The first one is called Link Balancer Controller(LBC). In this implementation, when a new data plane event is triggered, the information of the whole network will be sent by the Network Operating System(NOS) to the domain application instance. The information consists of both the status of the local controller and the updates from other controllers. However, continually processing so much update information will affect the performance, and thus another implementation called Separate State Link Balancer Controller(SSLBC) is presented. In SSLBC, only intra-domain network status is kept fresh, and the inter-domain changes are synchronized periodically. This enables the controller to choose a least loaded server with local information faster. Meanwhile, since controllers do not get inter-domain status in real time, this model is more robust to NOS staleness. 

Inspired by all these problems and predecessors' works, we propose a novel strategy to manipulate the traffic in the OpenFlow network framework. In our scheme, a multi-switch and multi-controller model will be established to simulate the OpenFlow network with devolved controllers in DCN. In this model, each switch has several availble controllers, where one of them is the master controller and the rests are slave controller. And each controller also manages several switches. All the controllers has an upper bound of its performance capacity, and would try to keep an appropriate utilization ratio by migrating switches between each other. Meanwhile, we will also bring up with the idea of 'value' or 'weight' in each switch, according to the physical distance between the switch and different controllers. That is to say, in our load balance scheme, we will not only consider how to minimize the difference of utilization ratio among all controllers, but also consider how to maximize the total value existed in the OpenFlow framework. In all, the contribution of the paper is as follows:
\begin{enumerate}
\item We established a load balancing model with capacity limits and values of OpenFlow network, and raised an algorithm to maximize the value as well as minimize the difference on ultilization ratio. The value of switches are decided by the physical distance between a certain controller.
\item We deployed both centralized and distributed algorithms presented by the predecessors in the OpenFlow framework, and correct the mistake in former experiments.
\item We test the algorithms in simulation environments of OpenFlow and discuss the process of the scheme and its practicality with information shown by the outcome. 
\end{enumerate}

The structure of this paper is organized as follows. Chapter 1 is an introduction of OpenFlow and some predecessing work on load balancing of OpenFlow. Chapter 2 is a brief introduction and discussion of Data Center Networks. Chapter 3 is a detailed presentation on the structure of OpenFlow framework and a comparision of current controllers. Chapter 4 mainly discusses the model established for load balancing with devolved controllers in OpenFlow, and introduces the algorithms used for solving this problem. Chapter 5 shows the outcome of experiments on the schemes, and will give a elaborate comparision and analysis on the outcomes. Chapter 6 is gives some opinions on future works. Chapter 7 is the related works, and chapter 8 concludes this paper.

